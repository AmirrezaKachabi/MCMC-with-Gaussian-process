{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c609d130-5c54-4983-a072-cf6ee1b32bde",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 24pt;\">Loading the dataset + Splitting it to train and test</span></strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e97f7-51cc-4f1a-b255-7dce13826536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import least_squares,minimize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import least_squares,minimize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('PDE_data.csv')\n",
    "\n",
    "# Separate input (X) and output (Y)\n",
    "inputs = data.iloc[:, :5].values\n",
    "outputs = data.iloc[:, 5:].values\n",
    "\n",
    "# Initialize MinMaxScaler for inputs\n",
    "input_scaler = MinMaxScaler()\n",
    "scaled_inputs = input_scaler.fit_transform(inputs)\n",
    "# Convert the scaled data to a PyTorch tensor\n",
    "scaled_inputs_tensor = torch.tensor(scaled_inputs, dtype=torch.float32)\n",
    "# Split the output data into 4 signals\n",
    "signal1 = outputs[:, :32]\n",
    "signal2 = outputs[:, 32:64]\n",
    "signal3 = outputs[:, 64:96]\n",
    "signal4 = outputs[:, 96:]\n",
    "\n",
    "# Initialize MinMaxScaler for outputs\n",
    "output_scaler = MinMaxScaler()\n",
    "\n",
    "# Scale each signal individually to the range of 0-1\n",
    "scaled_signal1 = output_scaler.fit_transform(signal1)\n",
    "scaled_signal2 = output_scaler.fit_transform(signal2)\n",
    "scaled_signal3 = output_scaler.fit_transform(signal3)\n",
    "scaled_signal4 = output_scaler.fit_transform(signal4)\n",
    "\n",
    "# Combine the scaled signals into a new output with 128 components\n",
    "scaled_outputs = np.concatenate((scaled_signal1, scaled_signal2, scaled_signal3, scaled_signal4), axis=1)\n",
    "\n",
    "# Convert the scaled output data to a PyTorch tensor\n",
    "scaled_outputs_tensor = torch.tensor(scaled_outputs, dtype=torch.float32)\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(scaled_inputs_tensor, scaled_outputs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the input and output data to PyTorch tensors\n",
    "train_x = torch.tensor(train_X, dtype=torch.float32)\n",
    "test_x = torch.tensor(test_X, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_Y, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_Y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c16dde-e3af-4744-9f15-4abd4b1ef313",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 24pt;\">Applying PCA to reduce outputs diminsion</span></strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4615f3f-db3c-4b2b-a963-45fa39958184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_reshaped = train_Y.reshape(-1, train_Y.shape[1])\n",
    "test_Y_reshaped = test_Y.reshape(-1, test_Y.shape[1])\n",
    "# Specify batch size\n",
    "batch_size = 50\n",
    "\n",
    "# Create IncrementalPCA object\n",
    "pca = IncrementalPCA(n_components=12)\n",
    "\n",
    "# Fit the model in batches\n",
    "for i in range(0, len(train_Y_reshaped), batch_size):\n",
    "    batch = train_Y_reshaped[i:i + batch_size, :]\n",
    "    pca.partial_fit(batch)\n",
    "\n",
    "# Transform training and testing data to reduced dimensions\n",
    "train_y_pca = pca.transform(train_Y_reshaped)\n",
    "test_y_pca = pca.transform(test_Y_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9563e6-ef41-48bc-873a-ca39500eacee",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 24pt;\">Creating the GP model ( multitask)</span></strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba49f18-36fe-4eb9-a723-2826e68313d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_Y_pca, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_Y_pca, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=12\n",
    "        )\n",
    "        \n",
    "        # Using Matern kernel instead of RBFKernel\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            gpytorch.kernels.MaternKernel(), num_tasks=12, rank=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=12)\n",
    "model = MultitaskGPModel(train_x, torch.Tensor(train_y_pca), likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57a10f15-95cc-4668-9004-5bfd755a6c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/1000 - Loss: 1.121\n",
      "Iter 51/1000 - Loss: -0.983\n",
      "Iter 101/1000 - Loss: -1.743\n",
      "Iter 151/1000 - Loss: -1.811\n",
      "Iter 201/1000 - Loss: -1.826\n",
      "Iter 251/1000 - Loss: -1.832\n",
      "Iter 301/1000 - Loss: -1.836\n",
      "Iter 351/1000 - Loss: -1.839\n",
      "Iter 401/1000 - Loss: -1.841\n",
      "Iter 451/1000 - Loss: -1.842\n",
      "Iter 501/1000 - Loss: -1.843\n",
      "Iter 551/1000 - Loss: -1.844\n",
      "Iter 601/1000 - Loss: -1.845\n",
      "Iter 651/1000 - Loss: -1.845\n",
      "Iter 701/1000 - Loss: -1.845\n",
      "Iter 751/1000 - Loss: -1.846\n",
      "Iter 801/1000 - Loss: -1.846\n",
      "Iter 851/1000 - Loss: -1.847\n",
      "Iter 901/1000 - Loss: -1.847\n",
      "Iter 951/1000 - Loss: -1.847\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 2 if smoke_test else 1000\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, torch.Tensor(train_y_pca))\n",
    "    loss.backward()\n",
    "    if (i%50==0):\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    \n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556faf6-95b0-4bc2-b1b1-4604dd36b20f",
   "metadata": {},
   "source": [
    "<h2 id=\"Importing-true-DATA\"><strong><span style=\"font-size: 24pt;\">Importing TRUE measured DATA</span></strong></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c41fd16d-8d97-41fc-be5b-f575d3e1740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the measured_data from the Excel file\n",
    "data_m = pd.read_csv('MEASURED_DATA.csv')\n",
    "#true_inputs_scaled = scaler.transform(new_inputs_tensor).flatten()\n",
    "true_outputs = data_m.iloc[:, :].values\n",
    "true_outputs=torch.Tensor(true_outputs)\n",
    "true_pressure_m = true_outputs[:,:2].cpu().numpy().flatten()\n",
    "true_flow1_m = true_outputs[:,2:34].cpu().numpy().flatten()\n",
    "true_flow2_m = true_outputs[:,34:66].cpu().numpy().flatten()\n",
    "true_area_m = true_outputs[:,66:98].cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0b048-073f-456c-9e31-c9e0c818993a",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 24pt;\">Comparing GP predictions with True test Data</span></strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216bcbe-eb0f-411a-a471-2dbc1e34f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set the model and likelihood to evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "num_samples = 5  # Number of samples to consider from test data\n",
    "\n",
    "# Make predictions on the test set using the model trained on the training data\n",
    "# MJC: Since GPs naturally incorporate uncertainty, I've added the plotting commands for that here\n",
    "with torch.no_grad(), gpytorch.settings.fast_computations():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    q = torch.Tensor(np.array(test_x[:num_samples], ndmin=2))  # Select the first 10 samples\n",
    "    predictions = model(q)\n",
    "    lower, upper = predictions.confidence_region()\n",
    "\n",
    "# Extract the means of the predictions for each output task\n",
    "means_pred_subset = predictions.mean.detach().numpy()\n",
    "\n",
    "# Inverse transform the predictions back to the original dimensionality of 32 features\n",
    "model_predictions_original_dim = pca.inverse_transform(means_pred_subset)\n",
    "\n",
    "# Define MinMaxScaler for each signal individually\n",
    "output_scaler1 = MinMaxScaler()\n",
    "output_scaler2 = MinMaxScaler()\n",
    "output_scaler3 = MinMaxScaler()\n",
    "output_scaler4 = MinMaxScaler()\n",
    "\n",
    "# Fit and transform each signal individually\n",
    "scaled_signal1 = output_scaler1.fit_transform(signal1)\n",
    "scaled_signal2 = output_scaler2.fit_transform(signal2)\n",
    "scaled_signal3 = output_scaler3.fit_transform(signal3)\n",
    "scaled_signal4 = output_scaler4.fit_transform(signal4)\n",
    "\n",
    "# Use the transformed predictions to calculate the confidence intervals\n",
    "lower_original_dim, upper_original_dim = pca.inverse_transform(lower.numpy()), pca.inverse_transform(upper.numpy())\n",
    "\n",
    "# Inverse transform the predictions back to the original scale\n",
    "original_dim_predictions = np.zeros_like(model_predictions_original_dim)\n",
    "\n",
    "# Loop over each signal and perform inverse transformation\n",
    "for i in range(4):\n",
    "    if i == 0:\n",
    "        original_dim_predictions[:, i*32:(i+1)*32] = output_scaler1.inverse_transform(model_predictions_original_dim[:, i*32:(i+1)*32])\n",
    "    elif i == 1:\n",
    "        original_dim_predictions[:, i*32:(i+1)*32] = output_scaler2.inverse_transform(model_predictions_original_dim[:, i*32:(i+1)*32])\n",
    "    elif i == 2:\n",
    "        original_dim_predictions[:, i*32:(i+1)*32] = output_scaler3.inverse_transform(model_predictions_original_dim[:, i*32:(i+1)*32])\n",
    "    else:\n",
    "        original_dim_predictions[:, i*32:(i+1)*32] = output_scaler4.inverse_transform(model_predictions_original_dim[:, i*32:(i+1)*32])\n",
    "\n",
    "# Inverse transform the actual test data back to the original scale, using different scalers for each signal\n",
    "selected_test_y_np = test_y[:num_samples].cpu().numpy()\n",
    "original_dim_test_y = np.zeros_like(selected_test_y_np)\n",
    "\n",
    "for i in range(4):\n",
    "    if i == 0:\n",
    "        original_dim_test_y[:, i*32:(i+1)*32] = output_scaler1.inverse_transform(selected_test_y_np[:, i*32:(i+1)*32])\n",
    "    elif i == 1:\n",
    "        original_dim_test_y[:, i*32:(i+1)*32] = output_scaler2.inverse_transform(selected_test_y_np[:, i*32:(i+1)*32])\n",
    "    elif i == 2:\n",
    "        original_dim_test_y[:, i*32:(i+1)*32] = output_scaler3.inverse_transform(selected_test_y_np[:, i*32:(i+1)*32])\n",
    "    else:\n",
    "        original_dim_test_y[:, i*32:(i+1)*32] = output_scaler4.inverse_transform(selected_test_y_np[:, i*32:(i+1)*32])\n",
    "\n",
    "# Define the y-axis titles for each signal\n",
    "y_axis_titles = ['MPA Pressure (mmHg)', 'LPA Flow (mL/s)', 'RPA Flow (mL/s)', 'MPA area (cm2)']\n",
    "\n",
    "# Loop over each sample\n",
    "for j in range(num_samples):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Initialize lists to store differences for each signal\n",
    "    differences = []\n",
    "    \n",
    "    # Loop over each signal\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        \n",
    "        # Plot model prediction\n",
    "        plt.plot(np.arange(32), original_dim_predictions[j, i*32:(i+1)*32], color='blue', alpha=0.5, label='GP')\n",
    "        \n",
    "        # Plot actual data as a curve\n",
    "        plt.plot(np.arange(32), original_dim_test_y[j, i*32:(i+1)*32], color='red', label='transfomed back from scaling')\n",
    "        \n",
    "        # Plot test_yy data as black x markers\n",
    "        plt.plot(np.arange(32), test_yy[j, i*32:(i+1)*32], 'kx', label='original signal')\n",
    "        \n",
    "        plt.ylabel(y_axis_titles[i])\n",
    "        plt.title(y_axis_titles[i])\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d7e82-3b26-4c65-9182-5701972ea105",
   "metadata": {},
   "source": [
    "<h4 id=\"OPTIMIZATION\"><span style=\"font-size: 24pt;\">OPTIMIZATION ( It's good to do OPT first. so we can have a good intial values for MCMC)&nbsp;<br /></span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cab69fc-ed8b-4ca5-8175-b2ca67f6427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function before using it in the optimization loop\n",
    "def OPT_SSE_realdata(q, true_pressure_data, true_flow1, true_flow2, true_area):\n",
    "    n_pts = 32\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    q = torch.Tensor([q])\n",
    "    predictions = model(q)\n",
    "    model_predictions = predictions.mean.detach().numpy().flatten()\n",
    "    model_predictions = pca.inverse_transform(model_predictions)\n",
    "\n",
    "    # Define MinMaxScaler for each signal individually\n",
    "    output_scaler1 = MinMaxScaler()\n",
    "    output_scaler2 = MinMaxScaler()\n",
    "    output_scaler3 = MinMaxScaler()\n",
    "    output_scaler4 = MinMaxScaler()\n",
    "\n",
    "    # Fit and transform each signal individually\n",
    "    scaled_signal1 = output_scaler1.fit_transform(signal1)\n",
    "    scaled_signal2 = output_scaler2.fit_transform(signal2)\n",
    "    scaled_signal3 = output_scaler3.fit_transform(signal3)\n",
    "    scaled_signal4 = output_scaler4.fit_transform(signal4)\n",
    "\n",
    "    # Inverse transform each signal back to the original space\n",
    "    model_predictions_original_dim = np.zeros((1, 128))  # Initialize array with correct dimensions\n",
    "    model_predictions_original_dim[:, :32] = output_scaler1.inverse_transform(model_predictions[:32].reshape(1, -1))\n",
    "    model_predictions_original_dim[:, 32:64] = output_scaler2.inverse_transform(model_predictions[32:64].reshape(1, -1))\n",
    "    model_predictions_original_dim[:, 64:96] = output_scaler3.inverse_transform(model_predictions[64:96].reshape(1, -1))\n",
    "    model_predictions_original_dim[:, 96:] = output_scaler4.inverse_transform(model_predictions[96:].reshape(1, -1))\n",
    "\n",
    "    # Extract pressure data from the first 32 components of model_predictions\n",
    "    p_predictions = model_predictions_original_dim[:, :32].flatten()\n",
    "\n",
    "    # Create a parameter containing only the max and min of the first 32 components\n",
    "    max_min_pressure_model = np.array([np.max(p_predictions), np.min(p_predictions)])\n",
    "\n",
    "    # Calculate MSE for each subgroup\n",
    "    mse_pressure = np.sum((true_pressure_data - max_min_pressure_model)**2)\n",
    "    mse_flow1 = np.sum((true_flow1 - model_predictions_original_dim[:, 32:64].flatten())**2)\n",
    "    mse_flow2 = np.sum((true_flow2 - model_predictions_original_dim[:, 64:96].flatten())**2)\n",
    "    mse_area = np.sum((true_area - model_predictions_original_dim[:, 96:].flatten())**2)\n",
    "    total_mse = mse_pressure + mse_flow1 + mse_flow2 + mse_area\n",
    "    print(mse_pressure,mse_flow1,mse_flow2,mse_area)\n",
    "\n",
    "    return total_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d65d1-7469-4be4-8ef2-d2e8aeab3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Define which dataset to use\n",
    "true_pressure_data = true_pressure_m.flatten()\n",
    "true_flow1_data = true_flow1_m.flatten()\n",
    "true_flow2_data = true_flow2_m.flatten()\n",
    "true_area = true_area_m.flatten()\n",
    "#print(true_pressure_data)\n",
    "# Initial guess\n",
    "# Define the bounds for the optimization\n",
    "X0_true = torch.Tensor([[0.24, 0.2, 0.98, 0.44, 0.51]])\n",
    "total_costs = []\n",
    "param_save = []\n",
    "bounds = [(0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0,1.0)]  \n",
    "\n",
    "for i in range(8):\n",
    "    # Generate random perturbation within ±10% of X0_2\n",
    "    perturbation = torch.rand(X0_true.size()) * 0.1 - 0.05  # ±10% range\n",
    "    \n",
    "    # Apply perturbation to create a new guess\n",
    "    new_guess = X0_true * (1 + perturbation)\n",
    "    \n",
    "    # Perform optimization with new_guess\n",
    "    results = minimize(OPT_SSE_realdata, new_guess, args=(true_pressure_data, true_flow1_data, true_flow2_data, true_area),\n",
    "                       method='powell', options={'disp': False, 'maxiter': 1000, 'gtol': 1e-6, 'maxcor': 10})\n",
    "    \n",
    "    # Calculate total_cost based on the optimization results \n",
    "    total_cost = results.fun\n",
    "    \n",
    "    # Append the total_cost to the list\n",
    "    total_costs.append(total_cost)\n",
    "    param_save.append(results.x)\n",
    "    \n",
    "    # Print indication for the start of the next iteration\n",
    "    if i < 7:\n",
    "        print(f\"Next iteration ({i+2}/8) starting...\")\n",
    "\n",
    "# Print all total_costs\n",
    "print(\"Total costs for each iteration:\", total_costs)\n",
    "print(total_costs)\n",
    "print(param_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed806ff-baf1-40d1-996a-e906857872bd",
   "metadata": {},
   "source": [
    "## MCMC for REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d1821-9717-4344-a391-2148e1d6c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymcmcstat\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "from scipy.optimize import least_squares,minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from pymcmcstat.MCMC import MCMC # This is the MCMC package we use\n",
    "from pymcmcstat.settings.DataStructure import DataStructure # This is the sub module that is useful for MCMC\n",
    "from dataclasses import dataclass\n",
    "import ast\n",
    "from pymcmcstat import MCMC\n",
    "import pymcmcstat\n",
    "from pymcmcstat.MCMC import MCMC\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pymcmcstat import mcmcplot\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f98ef-9c60-49a8-b63e-2164ebfdf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSE_realdata(q, data):\n",
    "    n_pts = 32\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    q = torch.Tensor([q])\n",
    "    predictions = model(q)\n",
    "    model_predictions = predictions.mean.detach().numpy().flatten()\n",
    "    model_predictions = pca.inverse_transform(model_predictions)\n",
    "\n",
    "    # Define MinMaxScaler for each signal individually\n",
    "    output_scaler1 = MinMaxScaler()\n",
    "    output_scaler2 = MinMaxScaler()\n",
    "    output_scaler3 = MinMaxScaler()\n",
    "    output_scaler4 = MinMaxScaler()\n",
    "\n",
    "    # Fit and transform each signal individually\n",
    "    scaled_signal1 = output_scaler1.fit_transform(signal1)\n",
    "    scaled_signal2 = output_scaler2.fit_transform(signal2)\n",
    "    scaled_signal3 = output_scaler3.fit_transform(signal3)\n",
    "    scaled_signal4 = output_scaler4.fit_transform(signal4)\n",
    "\n",
    "    # Inverse transform each signal back to the original space\n",
    "    model_predictions_original_dim = np.zeros((1, 128))  # Initialize array with correct dimensions\n",
    "    model_predictions_original_dim[:, :32] = output_scaler1.inverse_transform(model_predictions[:32].reshape(1, -1))\n",
    "    model_predictions_original_dim[:, 32:64] = output_scaler2.inverse_transform(model_predictions[32:64].reshape(1, -1))\n",
    "    model_predictions_original_dim[:, 64:96] = output_scaler3.inverse_transform(model_predictions[64:96].reshape(1, -1))\n",
    "    model_predictions_original_dim[:, 96:] = output_scaler4.inverse_transform(model_predictions[96:].reshape(1, -1))\n",
    "\n",
    "    # Extract pressure data from the first 32 components of model_predictions\n",
    "    p_predictions = model_predictions_original_dim[:, :32].flatten()\n",
    "\n",
    "    # Create a parameter containing only the max and min of the first 32 components\n",
    "    max_min_pressure_model = np.array([np.max(p_predictions), np.min(p_predictions)])\n",
    "\n",
    "    # Separate true data into subgroups\n",
    "    true_pressure_data = data.ydata[0].flatten()\n",
    "    true_max_min_p = np.array([(np.max(true_pressure_data), np.min(true_pressure_data))], dtype=type(true_pressure_data))\n",
    "    true_flow1_data = data.ydata[1].flatten()\n",
    "    true_flow2_data = data.ydata[2].flatten()\n",
    "    true_area_data = data.ydata[3].flatten()\n",
    "\n",
    "    # Calculate MSE for each subgroup\n",
    "    mse_pressure = np.sum((true_max_min_p - max_min_pressure_model)**2)\n",
    "    mse_flow1 = np.sum((true_flow1_data - model_predictions_original_dim[:, 32:64].flatten())**2)\n",
    "    mse_flow2 = np.sum((true_flow2_data - model_predictions_original_dim[:, 64:96].flatten())**2)\n",
    "    mse_area = np.sum((true_area_data - model_predictions_original_dim[:, 96:].flatten())**2)\n",
    "    total_mse = [mse_pressure, mse_flow1, mse_flow2, mse_area]\n",
    "\n",
    "    total_mse = np.transpose([mse_pressure, mse_flow1, mse_flow2, mse_area])\n",
    "\n",
    "    return total_mse\n",
    "\n",
    "# Full parameter set\n",
    "theta0 = dict(k = 0.14, gamma1 = 0.5, lrr1=0.68, gamma2 = 0.8, lrr2=0.51) # initial values which are inputes_meas_scaled\n",
    "theta0vec = list(theta0.values())\n",
    "bounds = dict(k = [0,1], gamma1 = [0,1], lrr1 = [0,1], gamma2 = [0,1], lrr2 = [0,1])\n",
    "names = ['k','gamma1','lrr1','gamma2','lrr2']\n",
    "longnames = ['$k$','$gamma1$','$lrr1$','$gamma2$','$lrr2$']\n",
    "\n",
    "data = DataStructure()\n",
    "\n",
    "## NOTE: using a different name here so that we don't overwrite results\n",
    "p_data = true_pressure_m\n",
    "q1_data = true_flow1_m\n",
    "q2_data = true_flow2_m\n",
    "area_data = true_area_m\n",
    "\n",
    "\n",
    "mcstat_pluto = MCMC()\n",
    "mcstat_pluto.data.add_data_set(np.linspace(0,1,2),p_data)\n",
    "mcstat_pluto.data.add_data_set(np.linspace(0,1,32),q1_data)\n",
    "mcstat_pluto.data.add_data_set(np.linspace(0,1,32),q2_data)\n",
    "mcstat_pluto.data.add_data_set(np.linspace(0,1,32),area_data)\n",
    "mse0 = MJC_SSE_realdata([0.14, 0.5, 0.68, 0.8, 0.51],mcstat_pluto.data) \n",
    "print(mse0)\n",
    "\n",
    "for ii, name in enumerate(names):\n",
    "    mcstat_pluto.parameters.add_model_parameter(\n",
    "    name = longnames[ii],\n",
    "    theta0=theta0[name],\n",
    "    minimum=bounds[name][0],\n",
    "    maximum=bounds[name][1])\n",
    "    \n",
    "mcstat_pluto.model_settings.define_model_settings(sos_function=SSE_realdata,N = np.array([2,32,32,32]))\n",
    "       \n",
    "mcstat_pluto.simulation_options.define_simulation_options(\n",
    "        nsimu=25000,\n",
    "        method='dram', # Options are 'mh' for metropolis hastings, 'am' for adaptive metroplois, and 'dram'\n",
    "        waitbar=True,\n",
    "        updatesigma=True) # Always set to true\n",
    "with torch.no_grad(), gpytorch.settings.fast_computations():\n",
    "    mcstat_pluto.run_simulation()\n",
    "import pickle\n",
    "# Calculate chain statistics\n",
    "chainstats_results = mcstat_pluto.chainstats(mcstat_pluto.simulation_results.results['chain'][:, :],\n",
    "                                              mcstat_pluto.simulation_results.results)\n",
    "# Save relevant attributes\n",
    "results_to_save = {\n",
    "    'simulation_results': mcstat_pluto.simulation_results.results,\n",
    "    'chainstats_results': chainstats_results,\n",
    "    # Add any other necessary data\n",
    "}\n",
    "\n",
    "# Save the results\n",
    "with open(\"Sample.pickle\", \"wb\") as m:\n",
    "    pickle.dump(results_to_save, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b3dd6-e2df-423f-81d9-bd6260240bb7",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 24pt;\">Posterior density</span></strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52931606-c611-464d-b91a-cc6ec8fd0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results\n",
    "with open(\"Sample.pickle\", \"rb\") as f:\n",
    "    saved_results = pickle.load(f)\n",
    "\n",
    "# Extract relevant data\n",
    "simulation_results = saved_results['simulation_results']\n",
    "chainstats_results = saved_results.get('chainstats_results')  # Check if chainstats_results exist\n",
    "\n",
    "# Assuming chain has shape (nsimu, npar) and names contains the parameter names\n",
    "names = ['k', 'gamma1', 'lrr1', 'gamma2', 'lrr2']  # Modify with actual parameter names\n",
    "\n",
    "# Plot density panel\n",
    "fig = mcmcplot.plot_density_panel(simulation_results['chain'][:, :], names)\n",
    "\n",
    "# Set the color of the density plot to blue\n",
    "for ax in fig.axes:\n",
    "    ax.get_lines()[0].set_color('red')\n",
    "\n",
    "# Set x-axis limits to 0-1 for all subplots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "\n",
    "# Calculate mode for each parameter and plot as vertical lines\n",
    "for ax, param_name, chain in zip(fig.axes, names, simulation_results['chain'][:, :].T):\n",
    "    kde = gaussian_kde(chain)\n",
    "    x = np.linspace(min(chain), max(chain), 1000)\n",
    "    density = kde(x)\n",
    "    mode = x[np.argmax(density)]\n",
    "    ax.axvline(mode, color='blue', linestyle='--', label=f'Mode: {mode:.2f}')\n",
    "    ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24bc8b-80ef-458f-a95d-6ca33bfaf20e",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 24pt;\">Correlations plots</span></strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045b6e8-8093-4ece-8bab-8ae9274281aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data\n",
    "results = saved_results['simulation_results']\n",
    "chain = results['chain']\n",
    "s2chain = results['s2chain']\n",
    "names = results['names']\n",
    "\n",
    "# Plot pairwise correlation panel\n",
    "plt.figure\n",
    "mcmcplot.plot_pairwise_correlation_panel(chain, names)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4826d5-f9a4-4052-943b-c8a0fe53baf9",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Plotting the GP predictions vs True data using posteriors</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cfe52d-e5f7-4f6e-a6da-a60e4215052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'X0_true' is mode valuse from the posterirors \n",
    "X0_true = torch.tensor([0.14, 0.01, 0.99, 0.52, 0.96], dtype=torch.float32) #[0.72, 0.42, 0.97, 0.05, 0.76\n",
    "\n",
    "# Reshape the new input to match the expected size\n",
    "X0_true = X0_true.view(1, -1)\n",
    "\n",
    "# Set the model and likelihood to evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_computations():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    q = torch.Tensor(np.array(X0_true, ndmin=2))\n",
    "    predictions = model(q)\n",
    "    model_predictions = predictions.mean.detach().numpy().flatten()\n",
    "    model_predictions = pca.inverse_transform(model_predictions)\n",
    "output_scaler1 = MinMaxScaler()\n",
    "output_scaler2 = MinMaxScaler()\n",
    "output_scaler3 = MinMaxScaler()\n",
    "output_scaler4 = MinMaxScaler()\n",
    "\n",
    "# Fit and transform each signal individually\n",
    "scaled_signal1 = output_scaler1.fit_transform(signal1)\n",
    "scaled_signal2 = output_scaler2.fit_transform(signal2)\n",
    "scaled_signal3 = output_scaler3.fit_transform(signal3)\n",
    "scaled_signal4 = output_scaler4.fit_transform(signal4)\n",
    "\n",
    "    # Inverse transform each signal back to the original space\n",
    "model_predictions_original_dim = np.zeros((1, 128))  # Initialize array with correct dimensions\n",
    "model_predictions_original_dim[:, :32] = output_scaler1.inverse_transform(model_predictions[:32].reshape(1, -1))\n",
    "model_predictions_original_dim[:, 32:64] = output_scaler2.inverse_transform(model_predictions[32:64].reshape(1, -1))\n",
    "model_predictions_original_dim[:, 64:96] = output_scaler3.inverse_transform(model_predictions[64:96].reshape(1, -1))\n",
    "model_predictions_original_dim[:, 96:] = output_scaler4.inverse_transform(model_predictions[96:].reshape(1, -1))\n",
    "\n",
    "# Actual measurements for MPA pressure (min, max) and LPA/RPA flow\n",
    "true_MPA_pressure_min = 34.5  # actual sys pressure\n",
    "true_MPA_pressure_max = 21  # actual dias pressure\n",
    "true_LPA_flow = true_flow1_m  # actual LPA flow signal\n",
    "true_RPA_flow = true_flow2_m\n",
    "true_MPA_area= true_area_m \n",
    "\n",
    "## GP predictions\n",
    "p_GP=model_predictions_original_dim[:, :32].flatten()\n",
    "LPA_flow_GP=model_predictions_original_dim[:, 32:64].flatten()\n",
    "RPA_flow_GP=model_predictions_original_dim[:, 64:96].flatten()\n",
    "a_GP=model_predictions_original_dim[:, 96:].flatten()\n",
    "# Plot the predictions for the single input along with actual measurements\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(141)\n",
    "plt.plot(p_GP, color='blue', alpha=0.8, label='GP')\n",
    "plt.axhline(y=true_MPA_pressure_min, color='red', linestyle='--',)\n",
    "plt.axhline(y=true_MPA_pressure_max, color='red', linestyle='--', label='Measured')\n",
    "plt.ylabel('MPA Pressure (mmHg)')\n",
    "plt.title('MPA pressure')\n",
    "plt.legend()\n",
    "plt.xticks([0, 16, 32], ['0', 'T/2', 'T'])\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.plot(LPA_flow_GP, color='blue', alpha=0.8, label='GP')\n",
    "plt.plot(true_LPA_flow, color='red', linestyle='--', label='Measured')\n",
    "plt.ylabel('LPA Flow (mL/s)')\n",
    "plt.title('LPA Flow')\n",
    "plt.legend()\n",
    "plt.xticks([0, 16, 32], ['0', 'T/2', 'T'])\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.plot(RPA_flow_GP, color='blue', alpha=0.8, label='GP')\n",
    "plt.plot(true_RPA_flow, color='red', linestyle='--', label='Measured')\n",
    "plt.ylabel('RPA Flow (mL/s)')\n",
    "plt.title('RPA Flow')\n",
    "plt.legend()\n",
    "plt.xticks([0, 16, 32], ['0', 'T/2', 'T'])\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.plot(a_GP, color='blue', alpha=0.8, label='GP')\n",
    "plt.plot(np.roll(true_area_m,1), color='red', linestyle='--', label='Measured')\n",
    "plt.ylabel('Area(cm2)')\n",
    "plt.title('MPA area')\n",
    "plt.legend()\n",
    "plt.xticks([0, 16, 32], ['0', 'T/2', 'T'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
